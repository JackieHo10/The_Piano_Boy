{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "from music21 import converter, instrument, note, chord\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "def train_network():\n",
    "    \"\"\" Train a Neural Network to generate music \"\"\"\n",
    "    notes = get_notes()\n",
    "\n",
    "    network_input, network_output = prepare_sequences(notes)\n",
    "\n",
    "    model = create_network(network_input)\n",
    "\n",
    "    train(model, network_input, network_output)\n",
    "    \n",
    "    \n",
    "def merge_notes(notes_dict, max_offset):\n",
    "            \n",
    "    ret = np.array([])\n",
    "    for i in np.arange(0, max_offset, 0.5):\n",
    "        pitches = np.zeros(88)\n",
    "        if i in notes_dict:            \n",
    "            for element in notes_dict[i]:            \n",
    "                if isinstance(element, note.Note):\n",
    "                    pitches[element.pitch.midi-21] = 1\n",
    "                else:\n",
    "                    for p in element.pitches:\n",
    "                        pitches[p.midi-21] = 1    \n",
    "        ret = np.append(ret, pitches)\n",
    "    return ret\n",
    "    \n",
    "\n",
    "def get_notes():\n",
    "    \"\"\" Get all the notes and chords from the midi files in the ./midi_songs directory \"\"\"\n",
    "    notes = np.array([])\n",
    "\n",
    "    for file in glob.glob(\"midi_songs/*.mid\"):\n",
    "        midi = converter.parse(file)\n",
    "\n",
    "        print(\"Parsing %s\" % file)\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "        notes_dict = {}\n",
    "        max_offset = 0\n",
    "        for element in notes_to_parse:        \n",
    "            if isinstance(element, note.Note) or isinstance(element, chord.Chord):  \n",
    "                if element.offset not in notes_dict:\n",
    "                    notes_dict[element.offset] = []\n",
    "                notes_dict[element.offset].append(element)\n",
    "                max_offset = element.offset    \n",
    "        ret = merge_notes(notes_dict, max_offset)\n",
    "        #print(len(ret))\n",
    "        notes = np.append(notes, ret)\n",
    "        #print(len(notes))\n",
    "    row = notes.size / 88\n",
    "    notes = notes.reshape(int(row), 88)\n",
    "    with open('data/notes', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "    \n",
    "    return notes\n",
    "\n",
    "def prepare_sequences(notes):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 100\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append(sequence_in)\n",
    "        network_output.append(sequence_out)\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "    #print(n_patterns)\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 88))\n",
    "    network_output = np.reshape(network_output, (n_patterns, 88))\n",
    "    return (network_input, network_output)\n",
    "\n",
    "def create_network(network_input):\n",
    "    \"\"\" create the structure of the neural network \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        512,\n",
    "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(512, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(512))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(88))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "    #model.add(Activation('softmax'))\n",
    "    #model.compile(loss='categorical_crossentropy', optimizer='rmsprop')\n",
    "    return model\n",
    "\n",
    "def train(model, network_input, network_output):\n",
    "    \"\"\" train the neural network \"\"\"\n",
    "    filepath = \"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath,\n",
    "        monitor='loss',\n",
    "        verbose=0,\n",
    "        save_best_only=True,\n",
    "        mode='min'\n",
    "    )\n",
    "    callbacks_list = [checkpoint]\n",
    "    #print(network_input.shape)\n",
    "    #print(network_output.shape)\n",
    "    model.fit(network_input, network_output, epochs=200, batch_size=64, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n = get_notes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing midi_songs/Ff7-Cinco.mid\n",
      "Parsing midi_songs/waltz_de_choco.mid\n",
      "Parsing midi_songs/Ff4-BattleLust.mid\n",
      "Parsing midi_songs/dontbeafraid.mid\n",
      "Parsing midi_songs/electric_de_chocobo.mid\n",
      "Parsing midi_songs/costadsol.mid\n",
      "Parsing midi_songs/Ff7-Jenova_Absolute.mid\n",
      "Parsing midi_songs/tifap.mid\n",
      "Parsing midi_songs/Still_Alive-1.mid\n",
      "Parsing midi_songs/bcm.mid\n",
      "Parsing midi_songs/ff7-mainmidi.mid\n",
      "Parsing midi_songs/ultros.mid\n",
      "Parsing midi_songs/0fithos.mid\n",
      "Parsing midi_songs/mining.mid\n",
      "Parsing midi_songs/Ff7-One_Winged.mid\n",
      "Parsing midi_songs/FFIX_Piano.mid\n",
      "Parsing midi_songs/balamb.mid\n",
      "Parsing midi_songs/caitsith.mid\n",
      "Parsing midi_songs/path_of_repentance.mid\n",
      "Parsing midi_songs/ff1battp.mid\n",
      "Parsing midi_songs/Finalfantasy6fanfarecomplete.mid\n",
      "Parsing midi_songs/Oppressed.mid\n",
      "Parsing midi_songs/relmstheme-piano.mid\n",
      "Parsing midi_songs/ff4-fight1.mid\n",
      "Parsing midi_songs/Kingdom_Hearts_Dearly_Beloved.mid\n",
      "Parsing midi_songs/Finalfantasy5gilgameshp.mid\n",
      "Parsing midi_songs/FF4.mid\n",
      "Parsing midi_songs/Final_Fantasy_Matouyas_Cave_Piano.mid\n",
      "Parsing midi_songs/Eternal_Harvest.mid\n",
      "Parsing midi_songs/thoughts.mid\n",
      "Parsing midi_songs/ff7themep.mid\n",
      "Parsing midi_songs/goldsaucer.mid\n",
      "Parsing midi_songs/Gold_Silver_Rival_Battle.mid\n",
      "Parsing midi_songs/Fyw_piano.mid\n",
      "Parsing midi_songs/EyesOnMePiano.mid\n",
      "Parsing midi_songs/BlueStone_LastDungeon.mid\n",
      "Parsing midi_songs/ff4-airship.mid\n",
      "Parsing midi_songs/sobf.mid\n",
      "Parsing midi_songs/ff4pclov.mid\n",
      "Parsing midi_songs/z_aeristhemepiano.mid\n",
      "Parsing midi_songs/HighwindTakestotheSkies.mid\n",
      "Parsing midi_songs/ahead_on_our_way_piano.mid\n",
      "Parsing midi_songs/Suteki_Da_Ne_(Piano_Version).mid\n",
      "Parsing midi_songs/lurk_in_dark.mid\n",
      "Parsing midi_songs/figaro.mid\n",
      "Parsing midi_songs/FFX_-_Ending_Theme_(Piano_Version)_-_by_Angel_FF.mid\n",
      "Parsing midi_songs/Kingdom_Hearts_Traverse_Town.mid\n",
      "Parsing midi_songs/cosmo.mid\n",
      "Parsing midi_songs/8.mid\n",
      "Parsing midi_songs/FFIII_Edgar_And_Sabin_Piano.mid\n",
      "Parsing midi_songs/FF3_Battle_(Piano).mid\n",
      "Parsing midi_songs/Zelda_Overworld.mid\n",
      "Parsing midi_songs/braska.mid\n",
      "Parsing midi_songs/sandy.mid\n",
      "Parsing midi_songs/ff4_piano_collections-main_theme.mid\n",
      "Parsing midi_songs/JENOVA.mid\n",
      "Parsing midi_songs/ViviinAlexandria.mid\n",
      "Parsing midi_songs/ff11_awakening_piano.mid\n",
      "Parsing midi_songs/Fiend_Battle_(Piano).mid\n",
      "Parsing midi_songs/Fierce_Battle_(Piano).mid\n",
      "Parsing midi_songs/great_war.mid\n",
      "Parsing midi_songs/FF6epitaph_piano.mid\n",
      "Parsing midi_songs/dayafter.mid\n",
      "Parsing midi_songs/ff6shap.mid\n",
      "Parsing midi_songs/VincentPiano.mid\n",
      "Parsing midi_songs/Life_Stream.mid\n",
      "Parsing midi_songs/FFIXQuMarshP.mid\n",
      "Parsing midi_songs/fortresscondor.mid\n",
      "Parsing midi_songs/FFVII_BATTLE.mid\n",
      "Parsing midi_songs/FF3_Third_Phase_Final_(Piano).mid\n",
      "Parsing midi_songs/AT.mid\n",
      "Parsing midi_songs/In_Zanarkand.mid\n",
      "Parsing midi_songs/ff8-lfp.mid\n",
      "Parsing midi_songs/Cids.mid\n",
      "Parsing midi_songs/sera_.mid\n",
      "Parsing midi_songs/FF8_Shuffle_or_boogie_pc.mid\n",
      "Parsing midi_songs/Rachel_Piano_tempofix.mid\n",
      "Parsing midi_songs/gerudo.mid\n",
      "Parsing midi_songs/ff4-town.mid\n",
      "Parsing midi_songs/DOS.mid\n",
      "Parsing midi_songs/OTD5YA.mid\n",
      "Parsing midi_songs/redwings.mid\n",
      "Parsing midi_songs/pkelite4.mid\n",
      "Parsing midi_songs/tpirtsd-piano.mid\n",
      "Parsing midi_songs/rufus.mid\n",
      "Parsing midi_songs/roseofmay-piano.mid\n",
      "Parsing midi_songs/traitor.mid\n",
      "Parsing midi_songs/Rydia_pc.mid\n",
      "Parsing midi_songs/ultimafro.mid\n",
      "Parsing midi_songs/decisive.mid\n",
      "Parsing midi_songs/Final_Fantasy_7_-_Judgement_Day_Piano.mid\n",
      "Parsing midi_songs/thenightmarebegins.mid\n",
      "Epoch 1/200\n",
      "46810/46810 [==============================] - 181s 4ms/step - loss: 0.0756\n",
      "Epoch 2/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0619\n",
      "Epoch 3/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0577\n",
      "Epoch 4/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0544\n",
      "Epoch 5/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0506\n",
      "Epoch 6/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0469\n",
      "Epoch 7/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0435\n",
      "Epoch 8/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0399\n",
      "Epoch 9/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0364\n",
      "Epoch 10/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0332\n",
      "Epoch 11/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0299\n",
      "Epoch 12/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0272\n",
      "Epoch 13/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0248\n",
      "Epoch 14/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0227\n",
      "Epoch 15/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0207\n",
      "Epoch 16/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0193\n",
      "Epoch 17/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0178\n",
      "Epoch 18/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0166\n",
      "Epoch 19/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0156\n",
      "Epoch 20/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0147\n",
      "Epoch 21/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0138\n",
      "Epoch 22/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0130\n",
      "Epoch 23/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0124\n",
      "Epoch 24/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0119\n",
      "Epoch 25/200\n",
      "46810/46810 [==============================] - 173s 4ms/step - loss: 0.0113\n",
      "Epoch 26/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0109\n",
      "Epoch 27/200\n",
      "46810/46810 [==============================] - 174s 4ms/step - loss: 0.0104\n",
      "Epoch 28/200\n",
      "46810/46810 [==============================] - 175s 4ms/step - loss: 0.0100\n",
      "Epoch 29/200\n",
      "46810/46810 [==============================] - 174s 4ms/step - loss: 0.0096\n",
      "Epoch 30/200\n",
      "46810/46810 [==============================] - 173s 4ms/step - loss: 0.0093\n",
      "Epoch 31/200\n",
      "46810/46810 [==============================] - 170s 4ms/step - loss: 0.0090\n",
      "Epoch 32/200\n",
      "46810/46810 [==============================] - 170s 4ms/step - loss: 0.0087\n",
      "Epoch 33/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0084\n",
      "Epoch 34/200\n",
      "46810/46810 [==============================] - 170s 4ms/step - loss: 0.0080\n",
      "Epoch 35/200\n",
      "46810/46810 [==============================] - 170s 4ms/step - loss: 0.0080\n",
      "Epoch 36/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0076\n",
      "Epoch 37/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0074\n",
      "Epoch 38/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0073\n",
      "Epoch 39/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0072\n",
      "Epoch 40/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0069\n",
      "Epoch 41/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0069\n",
      "Epoch 42/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0066\n",
      "Epoch 43/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0065\n",
      "Epoch 44/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0064\n",
      "Epoch 45/200\n",
      "46810/46810 [==============================] - 173s 4ms/step - loss: 0.0062\n",
      "Epoch 46/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0062\n",
      "Epoch 47/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0060\n",
      "Epoch 48/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0058\n",
      "Epoch 49/200\n",
      "46810/46810 [==============================] - 170s 4ms/step - loss: 0.0058\n",
      "Epoch 50/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0056\n",
      "Epoch 51/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0055\n",
      "Epoch 52/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0055\n",
      "Epoch 53/200\n",
      "46810/46810 [==============================] - 170s 4ms/step - loss: 0.0054\n",
      "Epoch 54/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0053\n",
      "Epoch 55/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0052\n",
      "Epoch 56/200\n",
      "46810/46810 [==============================] - 174s 4ms/step - loss: 0.0052\n",
      "Epoch 57/200\n",
      "46810/46810 [==============================] - 173s 4ms/step - loss: 0.0051\n",
      "Epoch 58/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0050\n",
      "Epoch 59/200\n",
      "46810/46810 [==============================] - 175s 4ms/step - loss: 0.0049\n",
      "Epoch 60/200\n",
      "46810/46810 [==============================] - 175s 4ms/step - loss: 0.0048\n",
      "Epoch 61/200\n",
      "46810/46810 [==============================] - 179s 4ms/step - loss: 0.0048\n",
      "Epoch 62/200\n",
      "46810/46810 [==============================] - 176s 4ms/step - loss: 0.0047\n",
      "Epoch 63/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0046\n",
      "Epoch 64/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0046\n",
      "Epoch 65/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0045\n",
      "Epoch 66/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0045\n",
      "Epoch 67/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0044\n",
      "Epoch 68/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0043\n",
      "Epoch 69/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0043\n",
      "Epoch 70/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0041\n",
      "Epoch 71/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0042\n",
      "Epoch 72/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0042\n",
      "Epoch 73/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0041\n",
      "Epoch 74/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0041\n",
      "Epoch 75/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0041\n",
      "Epoch 76/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0040\n",
      "Epoch 77/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0039\n",
      "Epoch 78/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0038\n",
      "Epoch 79/200\n",
      "46810/46810 [==============================] - 177s 4ms/step - loss: 0.0039\n",
      "Epoch 80/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0039\n",
      "Epoch 81/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0039\n",
      "Epoch 82/200\n",
      "46810/46810 [==============================] - 170s 4ms/step - loss: 0.0037\n",
      "Epoch 83/200\n",
      "46810/46810 [==============================] - 170s 4ms/step - loss: 0.0038\n",
      "Epoch 84/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0037\n",
      "Epoch 85/200\n",
      "46810/46810 [==============================] - 170s 4ms/step - loss: 0.0037\n",
      "Epoch 86/200\n",
      "46810/46810 [==============================] - 170s 4ms/step - loss: 0.0035\n",
      "Epoch 87/200\n",
      "46810/46810 [==============================] - 170s 4ms/step - loss: 0.0035\n",
      "Epoch 88/200\n",
      "46810/46810 [==============================] - 170s 4ms/step - loss: 0.0036\n",
      "Epoch 89/200\n",
      "46810/46810 [==============================] - 170s 4ms/step - loss: 0.0035\n",
      "Epoch 90/200\n",
      "46810/46810 [==============================] - 170s 4ms/step - loss: 0.0035\n",
      "Epoch 91/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0034\n",
      "Epoch 92/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0035\n",
      "Epoch 93/200\n",
      "46810/46810 [==============================] - 170s 4ms/step - loss: 0.0034\n",
      "Epoch 94/200\n",
      "46810/46810 [==============================] - 170s 4ms/step - loss: 0.0034\n",
      "Epoch 95/200\n",
      "46810/46810 [==============================] - 170s 4ms/step - loss: 0.0033\n",
      "Epoch 96/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0034\n",
      "Epoch 97/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0034\n",
      "Epoch 98/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0033\n",
      "Epoch 99/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0032\n",
      "Epoch 100/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0032\n",
      "Epoch 101/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0032\n",
      "Epoch 102/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0030\n",
      "Epoch 103/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0032\n",
      "Epoch 104/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0032\n",
      "Epoch 105/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0031\n",
      "Epoch 106/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0030\n",
      "Epoch 107/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0031\n",
      "Epoch 108/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0030\n",
      "Epoch 109/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0030\n",
      "Epoch 110/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0030\n",
      "Epoch 111/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0030\n",
      "Epoch 112/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0030\n",
      "Epoch 113/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0030\n",
      "Epoch 114/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0030\n",
      "Epoch 115/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0028\n",
      "Epoch 116/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0029\n",
      "Epoch 117/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0028\n",
      "Epoch 118/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0030\n",
      "Epoch 119/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0029\n",
      "Epoch 120/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0028\n",
      "Epoch 121/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0028\n",
      "Epoch 122/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0028\n",
      "Epoch 123/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0028\n",
      "Epoch 124/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0029\n",
      "Epoch 125/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0027\n",
      "Epoch 126/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0028\n",
      "Epoch 127/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0027\n",
      "Epoch 128/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0027\n",
      "Epoch 129/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0027\n",
      "Epoch 130/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0033\n",
      "Epoch 131/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0027\n",
      "Epoch 132/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0027\n",
      "Epoch 133/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0027\n",
      "Epoch 134/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0027\n",
      "Epoch 135/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0025\n",
      "Epoch 136/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0026\n",
      "Epoch 137/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0026\n",
      "Epoch 138/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0026\n",
      "Epoch 139/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0027\n",
      "Epoch 140/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0026\n",
      "Epoch 141/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0026\n",
      "Epoch 142/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0026\n",
      "Epoch 143/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0026\n",
      "Epoch 144/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0025\n",
      "Epoch 145/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0025\n",
      "Epoch 146/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0025\n",
      "Epoch 147/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0026\n",
      "Epoch 148/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0026\n",
      "Epoch 149/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0026\n",
      "Epoch 150/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0025\n",
      "Epoch 151/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0025\n",
      "Epoch 152/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0025\n",
      "Epoch 153/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0024\n",
      "Epoch 154/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0025\n",
      "Epoch 155/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0025\n",
      "Epoch 156/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0025\n",
      "Epoch 157/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0024\n",
      "Epoch 158/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0024\n",
      "Epoch 159/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0024\n",
      "Epoch 160/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0024\n",
      "Epoch 161/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0024\n",
      "Epoch 162/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0024\n",
      "Epoch 163/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0024\n",
      "Epoch 164/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0023\n",
      "Epoch 165/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0023\n",
      "Epoch 166/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0024\n",
      "Epoch 167/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0024\n",
      "Epoch 168/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0023\n",
      "Epoch 169/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0023\n",
      "Epoch 170/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0023\n",
      "Epoch 171/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0023\n",
      "Epoch 172/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0023\n",
      "Epoch 173/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0023\n",
      "Epoch 174/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0022\n",
      "Epoch 175/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0023\n",
      "Epoch 176/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0022\n",
      "Epoch 177/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0022\n",
      "Epoch 178/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0023\n",
      "Epoch 179/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0023\n",
      "Epoch 180/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0022\n",
      "Epoch 181/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0022\n",
      "Epoch 182/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0023\n",
      "Epoch 183/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0022\n",
      "Epoch 184/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0022\n",
      "Epoch 185/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0022\n",
      "Epoch 186/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0022\n",
      "Epoch 187/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0022\n",
      "Epoch 188/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0023\n",
      "Epoch 189/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0021\n",
      "Epoch 190/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0022\n",
      "Epoch 191/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0022\n",
      "Epoch 192/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0021\n",
      "Epoch 193/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0022\n",
      "Epoch 194/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0022\n",
      "Epoch 195/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0021\n",
      "Epoch 196/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0022\n",
      "Epoch 197/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0021\n",
      "Epoch 198/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0021\n",
      "Epoch 199/200\n",
      "46810/46810 [==============================] - 172s 4ms/step - loss: 0.0021\n",
      "Epoch 200/200\n",
      "46810/46810 [==============================] - 171s 4ms/step - loss: 0.0021\n"
     ]
    }
   ],
   "source": [
    "train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "from music21 import converter, instrument, note, chord\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This module generates notes for a midi file using the\n",
    "    trained neural network \"\"\"\n",
    "import glob\n",
    "import pickle\n",
    "import numpy as np\n",
    "from music21 import instrument, note, stream, chord, pitch\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Activation\n",
    "\n",
    "sequence_length = 100\n",
    "\n",
    "def generate():\n",
    "    \"\"\" Generate a piano midi file \"\"\"\n",
    "    notes = get_notes()\n",
    "    #load the notes used to train the model\n",
    "#     with open('data/notes_test', 'rb') as filepath:\n",
    "#         notes = pickle.load(filepath)\n",
    "\n",
    "    network_input, network_input_normalize = prepare_sequences(notes)\n",
    "    model = create_network(network_input_normalize)\n",
    "    start, prediction_output = generate_notes(model, network_input)\n",
    "    create_midi(prediction_output)\n",
    "    count = evaluate(prediction_output, start, notes)\n",
    "\n",
    "    \n",
    "def merge_notes(notes_dict, max_offset):\n",
    "            \n",
    "    ret = np.array([])\n",
    "    for i in np.arange(0, max_offset, 0.5):\n",
    "        pitches = np.zeros(88)\n",
    "        if i in notes_dict:            \n",
    "            for element in notes_dict[i]:            \n",
    "                if isinstance(element, note.Note):\n",
    "                    pitches[element.pitch.midi-21] = 1\n",
    "                else:\n",
    "                    for p in element.pitches:\n",
    "                        pitches[p.midi-21] = 1    \n",
    "        ret = np.append(ret, pitches)\n",
    "    return ret\n",
    "    \n",
    "\n",
    "def get_notes():\n",
    "    \"\"\" Get all the notes and chords from the midi files in the ./midi_songs directory \"\"\"\n",
    "    notes = np.array([])\n",
    "\n",
    "    for file in glob.glob(\"test_midi_songs/*.mid\"):\n",
    "        midi = converter.parse(file)\n",
    "\n",
    "        print(\"Parsing %s\" % file)\n",
    "\n",
    "        notes_to_parse = None\n",
    "\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "        notes_dict = {}\n",
    "        max_offset = 0\n",
    "        for element in notes_to_parse:        \n",
    "            if isinstance(element, note.Note) or isinstance(element, chord.Chord):  \n",
    "                if element.offset not in notes_dict:\n",
    "                    notes_dict[element.offset] = []\n",
    "                notes_dict[element.offset].append(element)\n",
    "                max_offset = element.offset    \n",
    "        ret = merge_notes(notes_dict, max_offset)\n",
    "        #print(len(ret))\n",
    "        notes = np.append(notes, ret)\n",
    "        #print(len(notes))\n",
    "    row = notes.size / 88\n",
    "    notes = notes.reshape(int(row), 88)\n",
    "#     with open('data/notes_test', 'wb') as filepath:\n",
    "#         pickle.dump(notes, filepath)\n",
    "    \n",
    "    return notes\n",
    "\n",
    "def prepare_sequences(notes):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    \n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append(sequence_in)\n",
    "        network_output.append(sequence_out)\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "    #print(n_patterns)\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    network_input_normalize = np.reshape(network_input, (n_patterns, sequence_length, 88))\n",
    "    return (network_input, network_input_normalize)\n",
    "\n",
    "def create_network(network_input):\n",
    "    \"\"\" create the structure of the neural network \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        512,\n",
    "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(512, return_sequences=True))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(LSTM(512))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(88))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='rmsprop')\n",
    "    \n",
    "    # Load the weights to each node\n",
    "    model.load_weights('weights-improvement-199-0.0021-bigger.hdf5')\n",
    "    return model\n",
    "\n",
    "def generate_notes(model, network_input):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # pick a random sequence from the input as a starting point for the prediction\n",
    "#     start = np.random.randint(0, len(network_input)-1)\n",
    "    start = 100\n",
    "#     print(len(network_input))\n",
    "    pattern = network_input[start]\n",
    "\n",
    "    prediction_output = []\n",
    "    threshold = 0.8\n",
    "    # generate 500 notes\n",
    "    for note_index in range(500):\n",
    "        prediction_input = np.reshape(pattern, (1, sequence_length, 88))\n",
    "        prediction = model.predict(prediction_input, verbose=0)[0]\n",
    "        prediction[prediction > threshold] = 1\n",
    "        prediction[prediction <= threshold] = 0   \n",
    "        pattern = np.append(pattern, prediction)[88:]    \n",
    "        #print(prediction)\n",
    "        prediction_output.append(prediction)\n",
    "    return (start, prediction_output)\n",
    "\n",
    "import time\n",
    "current_time = lambda: int(round(time.time()))\n",
    "\n",
    "def create_midi(prediction_output):\n",
    "    \"\"\" convert the output from the prediction to notes and create a midi file\n",
    "        from the notes \"\"\"\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "    #print(prediction_output)\n",
    "    # create note and chord objects based on the values generated by the model\n",
    "    for pattern in prediction_output:\n",
    "        indices = np.where(pattern > 0)[0]\n",
    "        if len(indices) > 0:\n",
    "            notes = []\n",
    "            for index in indices:\n",
    "                n = note.Note()\n",
    "                p = pitch.Pitch()\n",
    "                p.midi = index + 21\n",
    "                n.pitch = p\n",
    "                n.storedInstrument = instrument.Piano()\n",
    "                notes.append(n)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "       \n",
    "        # increase offset each iteration so that notes do not stack\n",
    "        offset += 0.25\n",
    "\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "\n",
    "    midi_stream.write('midi', fp='test_output_{}.mid'.format(current_time()))\n",
    "    \n",
    "def evaluate(prediction_output, start, notes):\n",
    "    count = 0;\n",
    "    for note_index in range(500):\n",
    "        for i in range(88):\n",
    "            if prediction_output[note_index][i]==notes[note_index+start+sequence_length][i]:\n",
    "                count += 1;\n",
    "    print(count/500/88)\n",
    "    return count;\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing test_midi_songs/balamb.mid\n",
      "Parsing test_midi_songs/0fithos.mid\n"
     ]
    }
   ],
   "source": [
    "generate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
